{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tools.torch_lib import *\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "device = cpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = gpu\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset/\"\n",
    "dataset_file_name = \"1D_3L_chart.csv\"\n",
    "plots_dir = \"plots/\"\n",
    "test_plots_dir = \"test_plots/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  AO/d  ro_formation  D/d  invasion_zone_ro       rok\n0           0   0.1      0.100000    1               0.1  0.997220\n1           1   0.1      0.125893    1               0.1  0.997328\n2           2   0.1      0.158489    1               0.1  0.997460\n3           3   0.1      0.199526    1               0.1  0.997621\n4           4   0.1      0.251189    1               0.1  0.997817",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>AO/d</th>\n      <th>ro_formation</th>\n      <th>D/d</th>\n      <th>invasion_zone_ro</th>\n      <th>rok</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.1</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.125893</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997328</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.1</td>\n      <td>0.158489</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.1</td>\n      <td>0.199526</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.1</td>\n      <td>0.251189</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997817</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir + dataset_file_name, index_col=False)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Unnamed: 0', 'AO/d', 'ro_formation', 'D/d', 'invasion_zone_ro', 'rok'], dtype='object')"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# print attribute's min max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0: min=0 max=5438690\n",
      "AO/d: min=0.1 max=1000.0\n",
      "ro_formation: min=0.1 max=10000.0\n",
      "D/d: min=1 max=51\n",
      "invasion_zone_ro: min=0.1 max=10000.0\n",
      "rok: min=0.0898074 max=37655.3\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{column}: min={df[column].min()} max={df[column].max()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "   AO/d  ro_formation  D/d  invasion_zone_ro       rok\n0   0.1      0.100000    1               0.1  0.997220\n1   0.1      0.125893    1               0.1  0.997328\n2   0.1      0.158489    1               0.1  0.997460\n3   0.1      0.199526    1               0.1  0.997621\n4   0.1      0.251189    1               0.1  0.997817",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AO/d</th>\n      <th>ro_formation</th>\n      <th>D/d</th>\n      <th>invasion_zone_ro</th>\n      <th>rok</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.1</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.1</td>\n      <td>0.125893</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997328</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.1</td>\n      <td>0.158489</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.1</td>\n      <td>0.199526</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.1</td>\n      <td>0.251189</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.997817</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_to_drop = ['Unnamed: 0']\n",
    "df.drop(attributes_to_drop, axis=1, inplace=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add dataframe transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "inputs = np.array(['AO/d', 'ro_formation', 'invasion_zone_ro', 'D/d'])\n",
    "outputs = np.array(['rok']) # 'A02M01N' dropped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "             AO/d  ro_formation       D/d  invasion_zone_ro       rok\n0       -1.690309     -1.698416 -1.698416         -1.698416 -0.628110\n1       -1.690309     -1.630478 -1.698416         -1.698416 -0.628093\n2       -1.690309     -1.562543 -1.698416         -1.698416 -0.628073\n3       -1.690309     -1.494606 -1.698416         -1.698416 -0.628048\n4       -1.690309     -1.426669 -1.698416         -1.698416 -0.628017\n...           ...           ...       ...               ...       ...\n5438686  1.690309      1.426669  1.698416          1.698416  0.666086\n5438687  1.690309      1.494606  1.698416          1.698416  0.702592\n5438688  1.690309      1.562542  1.698416          1.698416  0.739367\n5438689  1.690309      1.630479  1.698416          1.698416  0.776478\n5438690  1.690309      1.698416  1.698416          1.698416  0.814011\n\n[5438691 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AO/d</th>\n      <th>ro_formation</th>\n      <th>D/d</th>\n      <th>invasion_zone_ro</th>\n      <th>rok</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.690309</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-0.628110</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.690309</td>\n      <td>-1.630478</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-0.628093</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.690309</td>\n      <td>-1.562543</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-0.628073</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.690309</td>\n      <td>-1.494606</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-0.628048</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.690309</td>\n      <td>-1.426669</td>\n      <td>-1.698416</td>\n      <td>-1.698416</td>\n      <td>-0.628017</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5438686</th>\n      <td>1.690309</td>\n      <td>1.426669</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>0.666086</td>\n    </tr>\n    <tr>\n      <th>5438687</th>\n      <td>1.690309</td>\n      <td>1.494606</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>0.702592</td>\n    </tr>\n    <tr>\n      <th>5438688</th>\n      <td>1.690309</td>\n      <td>1.562542</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>0.739367</td>\n    </tr>\n    <tr>\n      <th>5438689</th>\n      <td>1.690309</td>\n      <td>1.630479</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>0.776478</td>\n    </tr>\n    <tr>\n      <th>5438690</th>\n      <td>1.690309</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>1.698416</td>\n      <td>0.814011</td>\n    </tr>\n  </tbody>\n</table>\n<p>5438691 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logarithmic_columns = ['ro_formation', 'invasion_zone_ro', 'AO/d']\n",
    "# normalize data ('min/max' normalization):\n",
    "interval_th = [-1, 1]     # normalization interval for 'th' activation function\n",
    "interval_sigmoid = [0, 1] # normalization interval for 'sigmoid' activation function\n",
    "normalize_interval = interval_th\n",
    "\n",
    "attributes_transform_dict = {}\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# transform output attributes:\n",
    "for output_attr in outputs:\n",
    "    attr_transformer = attributes_transform_dict[output_attr] = AttributeTransformer(df_transformed[output_attr].to_numpy())\n",
    "\n",
    "    # logarithmic transform\n",
    "    forward, backward = np.log, np.exp\n",
    "    df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "    # scaling transform\n",
    "    forward, backward = get_standard_scaler_transform(attr_transformer.data)\n",
    "    df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "    # # normalize transform\n",
    "    forward, backward = get_normalize_transforms(attr_transformer.data, normalize_interval)\n",
    "    df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "\n",
    "# logarithm resistance:\n",
    "for col in logarithmic_columns:\n",
    "    if col in outputs:\n",
    "        continue\n",
    "    df_transformed[col] = df_transformed[col].apply(np.log)\n",
    "\n",
    "# add normalization\n",
    "for attribute in df_transformed.columns:\n",
    "    if attribute in outputs:\n",
    "        continue\n",
    "    transform, _ = get_standard_scaler_transform(df_transformed[attribute].to_numpy())\n",
    "    df_transformed[attribute] = transform(df_transformed[attribute].to_numpy())\n",
    "\n",
    "    transform, _ = get_normalize_transforms(df_transformed[attribute].to_numpy(), normalize_interval)\n",
    "    df_transformed[attribute] = transform(df_transformed[attribute].to_numpy())\n",
    "\n",
    "df_transformed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Datasets and create dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def print_inference_statistic(attributes, df_):\n",
    "    means = []\n",
    "    stds = []\n",
    "    mins = []\n",
    "    maxes = []\n",
    "\n",
    "    for column in attributes:\n",
    "        col_data = df_[column].to_numpy()\n",
    "\n",
    "        if column in logarithmic_columns or column in outputs:\n",
    "            col_data = np.log(col_data)  # first transform - log\n",
    "\n",
    "        # col_mean = np.mean(col_data)\n",
    "        # col_std = np.std(col_data)\n",
    "\n",
    "        # means.append(col_mean)\n",
    "        # stds.append(col_std)\n",
    "        #\n",
    "        # col_data = (col_data - col_mean) / col_std\n",
    "\n",
    "        mins.append(np.min(col_data))\n",
    "        maxes.append(np.max(col_data))\n",
    "\n",
    "    # print(f\"means={means}\")\n",
    "    # print(f\"stds={stds}\")\n",
    "    print(f\"mins={mins}\")\n",
    "    print(f\"maxes={maxes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins=[-2.3025850929940455, -2.3025850929940455, -2.3025850929940455, 1]\n",
      "maxes=[6.907755278982137, 9.210340371976184, 9.210340371976184, 51]\n"
     ]
    }
   ],
   "source": [
    "print_inference_statistic(inputs, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins=[-2.4100879017239056]\n",
      "maxes=[10.536228993573161]\n"
     ]
    }
   ],
   "source": [
    "print_inference_statistic(outputs, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, df_, inputs, outputs, device):\n",
    "        self.df = df_\n",
    "        self.inputs = torch.from_numpy(df_[inputs].to_numpy()).float().to(device)\n",
    "        self.outputs = torch.from_numpy(df_[outputs].to_numpy()).float().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "        return item, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "train_df, test_df = train_test_split(df_transformed, shuffle=True, test_size=0.3)\n",
    "test_df, validation_df = train_test_split(test_df, shuffle=True, test_size=0.33)\n",
    "\n",
    "train_dataset = SimpleDataset(train_df, inputs, outputs, device)\n",
    "test_dataset = SimpleDataset(test_df, inputs, outputs, device)\n",
    "validation_dataset = SimpleDataset(validation_df, inputs, outputs, device)\n",
    "full_dataset = SimpleDataset(df_transformed, inputs, outputs, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "full_dataset_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "class WeightedMAE(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedMAE, self).__init__()\n",
    "        self.mae = nn.L1Loss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        weighted_inputs = inputs * self.weights\n",
    "\n",
    "        return self.mae(weighted_inputs, targets)\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.weights = self.weights.to(device)\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        module_list = []\n",
    "        for i in range(layers_count - 1):\n",
    "            module_list.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n",
    "        module_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "\n",
    "        activations_list = []\n",
    "        for i in range(layers_count):\n",
    "            activations_list.append(activations[act_str_list[i]])\n",
    "\n",
    "        self.linears = nn.ModuleList(module_list)\n",
    "        self.activations = nn.ModuleList(activations_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act in zip(self.linears, self.activations):\n",
    "            y = lin(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class LinearLNormModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        linears_list = []\n",
    "        layers_norm_list = []\n",
    "\n",
    "        for i in range(layers_count - 1):\n",
    "            in_features, out_features = layers_dims[i], layers_dims[i + 1]\n",
    "            linears_list.append(nn.Linear(in_features, out_features))\n",
    "            layers_norm_list.append(nn.LayerNorm(out_features))\n",
    "        # add last layer\n",
    "        linears_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "        layers_norm_list.append(nn.LayerNorm(output_dim))\n",
    "\n",
    "        self.linears = nn.ModuleList(linears_list)\n",
    "        self.activations = nn.ModuleList([activations[act_str_list[i]] for i in range(len(act_str_list))])\n",
    "        self.layer_normalizations = nn.ModuleList(layers_norm_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act, norm in zip(self.linears, self.activations, self.layer_normalizations):\n",
    "            y = lin(y)\n",
    "            y = norm(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# add batch normalization\n",
    "class LinearBNormModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        linears_list = []\n",
    "        batch_norm_list = []\n",
    "\n",
    "        for i in range(layers_count - 1):\n",
    "            in_features, out_features = layers_dims[i], layers_dims[i + 1]\n",
    "            linears_list.append(nn.Linear(in_features, out_features))\n",
    "            batch_norm_list.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "        linears_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "        batch_norm_list.append(nn.BatchNorm1d(output_dim))\n",
    "\n",
    "        activations_list = []\n",
    "        for i in range(layers_count):\n",
    "            activations_list.append(activations[act_str_list[i]])\n",
    "\n",
    "        self.linears = nn.ModuleList(linears_list)\n",
    "        self.activations = nn.ModuleList(activations_list)\n",
    "        self.batch_normalizations = nn.ModuleList(batch_norm_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act, norm in zip(self.linears, self.activations, self.batch_normalizations):\n",
    "            y = lin(y)\n",
    "            y = norm(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearModel(\n  (linears): ModuleList(\n    (0): Linear(in_features=4, out_features=40, bias=True)\n    (1): Linear(in_features=40, out_features=150, bias=True)\n    (2): Linear(in_features=150, out_features=1500, bias=True)\n    (3): Linear(in_features=1500, out_features=150, bias=True)\n    (4): Linear(in_features=150, out_features=10, bias=True)\n    (5): Linear(in_features=10, out_features=1, bias=True)\n  )\n  (activations): ModuleList(\n    (0-5): 6 x LeakyReLU(negative_slope=0.01)\n  )\n)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = [len(inputs), 40, 150, 1500, 150, 10]\n",
    "layers_count = len(layers_dims)\n",
    "activations_string_list = ['leaky-relu' for i in range(layers_count)]\n",
    "#activations_string_list[-1] = 'sigmoid'\n",
    "\n",
    "linear_model = LinearModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "#linear_bn_model = LinearBNormModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "#linear_ln_model = LinearLNormModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "\n",
    "model = linear_model\n",
    "model_name = \"linear_model\"\n",
    "linear_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epoch_count = 350\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#loss_function = WeightedMAE(torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], dtype=float))\n",
    "loss_function = nn.L1Loss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; train loss=0.071461; validation loss=0.020676\n",
      "Epoch: 1; train loss=0.013553; validation loss=0.011162\n",
      "Epoch: 2; train loss=0.009789; validation loss=0.009216\n",
      "Epoch: 3; train loss=0.008194; validation loss=0.007427\n",
      "Epoch: 4; train loss=0.007304; validation loss=0.010042\n",
      "Epoch: 5; train loss=0.006675; validation loss=0.007145\n",
      "Epoch: 6; train loss=0.006177; validation loss=0.005659\n",
      "Epoch: 7; train loss=0.005831; validation loss=0.005562\n",
      "Epoch: 8; train loss=0.005502; validation loss=0.005046\n",
      "Epoch: 9; train loss=0.005181; validation loss=0.004873\n",
      "Epoch: 10; train loss=0.004942; validation loss=0.005154\n",
      "Epoch: 11; train loss=0.004718; validation loss=0.004481\n",
      "Epoch: 12; train loss=0.004537; validation loss=0.004241\n",
      "Epoch: 13; train loss=0.004374; validation loss=0.004894\n",
      "Epoch: 14; train loss=0.004271; validation loss=0.004161\n",
      "Epoch: 15; train loss=0.004088; validation loss=0.004084\n",
      "Epoch: 16; train loss=0.003952; validation loss=0.004775\n",
      "Epoch: 17; train loss=0.003884; validation loss=0.003533\n",
      "Epoch: 18; train loss=0.003730; validation loss=0.003584\n",
      "Epoch: 19; train loss=0.003637; validation loss=0.003605\n",
      "Epoch: 20; train loss=0.003552; validation loss=0.003287\n",
      "Epoch: 21; train loss=0.003458; validation loss=0.003303\n",
      "Epoch: 22; train loss=0.003406; validation loss=0.003610\n",
      "Epoch: 23; train loss=0.003339; validation loss=0.003182\n",
      "Epoch: 24; train loss=0.003233; validation loss=0.003009\n",
      "Epoch: 25; train loss=0.003152; validation loss=0.003605\n",
      "Epoch: 26; train loss=0.003162; validation loss=0.003198\n",
      "Epoch: 27; train loss=0.003030; validation loss=0.002644\n",
      "Epoch: 28; train loss=0.002989; validation loss=0.003154\n",
      "Epoch: 29; train loss=0.002924; validation loss=0.003303\n",
      "Epoch: 30; train loss=0.002884; validation loss=0.002827\n",
      "Epoch: 31; train loss=0.002813; validation loss=0.002601\n",
      "Epoch: 32; train loss=0.002738; validation loss=0.003084\n",
      "Epoch: 33; train loss=0.002704; validation loss=0.002544\n",
      "Epoch: 34; train loss=0.002648; validation loss=0.002908\n",
      "Epoch: 35; train loss=0.002572; validation loss=0.002799\n",
      "Epoch: 36; train loss=0.002534; validation loss=0.003116\n",
      "Epoch: 37; train loss=0.002505; validation loss=0.003031\n",
      "Epoch: 38; train loss=0.002402; validation loss=0.002329\n",
      "Epoch: 39; train loss=0.002391; validation loss=0.002845\n",
      "Epoch: 40; train loss=0.002327; validation loss=0.002203\n",
      "Epoch: 41; train loss=0.002301; validation loss=0.002909\n",
      "Epoch: 42; train loss=0.002239; validation loss=0.002170\n",
      "Epoch: 43; train loss=0.002197; validation loss=0.002106\n",
      "Epoch: 44; train loss=0.002182; validation loss=0.002338\n",
      "Epoch: 45; train loss=0.002087; validation loss=0.002559\n",
      "Epoch: 46; train loss=0.002078; validation loss=0.001971\n",
      "Epoch: 47; train loss=0.002034; validation loss=0.002786\n",
      "Epoch: 48; train loss=0.002001; validation loss=0.001690\n",
      "Epoch: 49; train loss=0.001943; validation loss=0.002050\n",
      "Epoch: 50; train loss=0.001913; validation loss=0.003991\n",
      "Epoch: 51; train loss=0.001895; validation loss=0.001766\n",
      "Epoch: 52; train loss=0.001870; validation loss=0.001692\n",
      "Epoch: 53; train loss=0.001840; validation loss=0.001737\n",
      "Epoch: 54; train loss=0.001788; validation loss=0.001803\n",
      "Epoch: 55; train loss=0.001799; validation loss=0.001626\n",
      "Epoch: 56; train loss=0.001725; validation loss=0.001866\n",
      "Epoch: 57; train loss=0.001742; validation loss=0.002000\n",
      "Epoch: 58; train loss=0.001713; validation loss=0.001677\n",
      "Epoch: 59; train loss=0.001669; validation loss=0.002365\n",
      "Epoch: 60; train loss=0.001681; validation loss=0.002052\n",
      "Epoch: 61; train loss=0.001697; validation loss=0.001378\n",
      "Epoch: 62; train loss=0.001624; validation loss=0.001618\n",
      "Epoch: 63; train loss=0.001627; validation loss=0.001649\n",
      "Epoch: 64; train loss=0.001590; validation loss=0.001538\n",
      "Epoch: 65; train loss=0.001587; validation loss=0.001435\n",
      "Epoch: 66; train loss=0.001573; validation loss=0.002120\n",
      "Epoch: 67; train loss=0.001540; validation loss=0.001659\n",
      "Epoch: 68; train loss=0.001555; validation loss=0.001492\n",
      "Epoch: 69; train loss=0.001506; validation loss=0.001303\n",
      "Epoch: 70; train loss=0.001507; validation loss=0.001402\n",
      "Epoch: 71; train loss=0.001522; validation loss=0.001796\n",
      "Epoch: 72; train loss=0.001501; validation loss=0.001743\n",
      "Epoch: 73; train loss=0.001462; validation loss=0.001835\n",
      "Epoch: 74; train loss=0.001473; validation loss=0.001551\n",
      "Epoch: 75; train loss=0.001457; validation loss=0.001412\n",
      "Epoch: 76; train loss=0.001470; validation loss=0.001775\n",
      "Epoch: 77; train loss=0.001436; validation loss=0.001316\n",
      "Epoch: 78; train loss=0.001430; validation loss=0.001286\n",
      "Epoch: 79; train loss=0.001421; validation loss=0.001896\n",
      "Epoch: 80; train loss=0.001421; validation loss=0.001695\n",
      "Epoch: 81; train loss=0.001411; validation loss=0.001302\n",
      "Epoch: 82; train loss=0.001390; validation loss=0.001133\n",
      "Epoch: 83; train loss=0.001382; validation loss=0.001234\n",
      "Epoch: 84; train loss=0.001385; validation loss=0.001581\n",
      "Epoch: 85; train loss=0.001393; validation loss=0.001706\n",
      "Epoch: 86; train loss=0.001374; validation loss=0.001356\n",
      "Epoch: 87; train loss=0.001354; validation loss=0.001482\n",
      "Epoch: 88; train loss=0.001347; validation loss=0.001558\n",
      "Epoch: 89; train loss=0.001342; validation loss=0.001025\n",
      "Epoch: 90; train loss=0.001342; validation loss=0.001280\n",
      "Epoch: 91; train loss=0.001332; validation loss=0.001600\n",
      "Epoch: 92; train loss=0.001322; validation loss=0.001117\n",
      "Epoch: 93; train loss=0.001298; validation loss=0.001872\n",
      "Epoch: 94; train loss=0.001330; validation loss=0.001016\n",
      "Epoch: 95; train loss=0.001306; validation loss=0.001631\n",
      "Epoch: 96; train loss=0.001302; validation loss=0.001343\n",
      "Epoch: 97; train loss=0.001284; validation loss=0.001602\n",
      "Epoch: 98; train loss=0.001297; validation loss=0.001546\n",
      "Epoch: 99; train loss=0.001266; validation loss=0.001623\n",
      "Epoch: 100; train loss=0.001254; validation loss=0.001724\n",
      "Epoch: 101; train loss=0.001275; validation loss=0.001435\n",
      "Epoch: 102; train loss=0.001254; validation loss=0.001375\n",
      "Epoch: 103; train loss=0.001270; validation loss=0.001247\n",
      "Epoch: 104; train loss=0.001279; validation loss=0.001114\n",
      "Epoch: 105; train loss=0.001257; validation loss=0.001033\n",
      "Epoch: 106; train loss=0.001238; validation loss=0.001177\n",
      "Epoch: 107; train loss=0.001273; validation loss=0.001312\n",
      "Epoch: 108; train loss=0.001241; validation loss=0.001254\n",
      "Epoch: 109; train loss=0.001218; validation loss=0.001356\n",
      "Epoch: 110; train loss=0.001233; validation loss=0.001239\n",
      "Epoch: 111; train loss=0.001221; validation loss=0.001246\n",
      "Epoch: 112; train loss=0.001260; validation loss=0.001175\n",
      "Epoch: 113; train loss=0.001233; validation loss=0.001408\n",
      "Epoch: 114; train loss=0.001232; validation loss=0.001780\n",
      "Epoch: 115; train loss=0.001223; validation loss=0.001787\n",
      "Epoch: 116; train loss=0.001220; validation loss=0.001447\n",
      "Epoch: 117; train loss=0.001197; validation loss=0.001456\n",
      "Epoch: 118; train loss=0.001197; validation loss=0.001611\n",
      "Epoch: 119; train loss=0.001205; validation loss=0.001149\n",
      "Epoch: 120; train loss=0.001195; validation loss=0.000910\n",
      "Epoch: 121; train loss=0.001196; validation loss=0.001662\n",
      "Epoch: 122; train loss=0.001216; validation loss=0.001057\n",
      "Epoch: 123; train loss=0.001188; validation loss=0.001207\n",
      "Epoch: 124; train loss=0.001197; validation loss=0.001531\n",
      "Epoch: 125; train loss=0.001184; validation loss=0.001031\n",
      "Epoch: 126; train loss=0.001202; validation loss=0.001158\n",
      "Epoch: 127; train loss=0.001172; validation loss=0.001013\n",
      "Epoch: 128; train loss=0.001170; validation loss=0.001572\n",
      "Epoch: 129; train loss=0.001185; validation loss=0.001605\n",
      "Epoch: 130; train loss=0.001180; validation loss=0.001612\n",
      "Epoch: 131; train loss=0.001157; validation loss=0.001123\n",
      "Epoch: 132; train loss=0.001238; validation loss=0.001484\n",
      "Epoch: 133; train loss=0.001146; validation loss=0.001154\n",
      "Epoch: 134; train loss=0.001146; validation loss=0.001497\n",
      "Epoch: 135; train loss=0.001173; validation loss=0.001072\n",
      "Epoch: 136; train loss=0.001131; validation loss=0.001212\n",
      "Epoch: 137; train loss=0.001148; validation loss=0.001256\n",
      "Epoch: 138; train loss=0.001169; validation loss=0.001265\n",
      "Epoch: 139; train loss=0.001142; validation loss=0.001210\n",
      "Epoch: 140; train loss=0.001160; validation loss=0.001131\n",
      "Epoch: 141; train loss=0.001126; validation loss=0.001045\n",
      "Epoch: 142; train loss=0.001125; validation loss=0.001340\n",
      "Epoch: 143; train loss=0.001133; validation loss=0.002299\n",
      "Epoch: 144; train loss=0.001179; validation loss=0.001495\n",
      "Epoch: 145; train loss=0.001114; validation loss=0.001743\n",
      "Epoch: 146; train loss=0.001128; validation loss=0.001351\n",
      "Epoch: 147; train loss=0.001162; validation loss=0.001423\n",
      "Epoch: 148; train loss=0.001126; validation loss=0.001544\n",
      "Epoch: 149; train loss=0.001115; validation loss=0.001073\n",
      "Epoch: 150; train loss=0.001128; validation loss=0.001947\n",
      "Epoch: 151; train loss=0.001102; validation loss=0.000927\n",
      "Epoch: 152; train loss=0.001126; validation loss=0.001466\n",
      "Epoch: 153; train loss=0.001101; validation loss=0.001362\n",
      "Epoch: 154; train loss=0.001121; validation loss=0.001292\n",
      "Epoch: 155; train loss=0.001101; validation loss=0.001142\n",
      "Epoch: 156; train loss=0.001108; validation loss=0.001479\n",
      "Epoch: 157; train loss=0.001088; validation loss=0.001442\n",
      "Epoch: 158; train loss=0.001099; validation loss=0.001218\n",
      "Epoch: 159; train loss=0.001109; validation loss=0.001318\n",
      "Epoch: 160; train loss=0.001108; validation loss=0.000849\n",
      "Epoch: 161; train loss=0.001087; validation loss=0.001393\n",
      "Epoch: 162; train loss=0.001113; validation loss=0.000998\n",
      "Epoch: 163; train loss=0.001116; validation loss=0.000988\n",
      "Epoch: 164; train loss=0.001116; validation loss=0.001336\n",
      "Epoch: 165; train loss=0.001092; validation loss=0.000863\n",
      "Epoch: 166; train loss=0.001111; validation loss=0.001624\n",
      "Epoch: 167; train loss=0.001086; validation loss=0.001072\n",
      "Epoch: 168; train loss=0.001109; validation loss=0.001305\n",
      "Epoch: 169; train loss=0.001108; validation loss=0.001423\n",
      "Epoch: 170; train loss=0.001077; validation loss=0.001610\n",
      "Epoch: 171; train loss=0.001082; validation loss=0.000886\n",
      "Epoch: 172; train loss=0.001086; validation loss=0.000897\n",
      "Epoch: 173; train loss=0.001088; validation loss=0.001187\n",
      "Epoch: 174; train loss=0.001128; validation loss=0.000904\n",
      "Epoch: 175; train loss=0.001072; validation loss=0.000878\n",
      "Epoch: 176; train loss=0.001072; validation loss=0.001229\n",
      "Epoch: 177; train loss=0.001083; validation loss=0.000945\n",
      "Epoch: 178; train loss=0.001062; validation loss=0.001043\n",
      "Epoch: 179; train loss=0.001109; validation loss=0.001073\n",
      "Epoch: 180; train loss=0.001069; validation loss=0.000990\n",
      "Epoch: 181; train loss=0.001060; validation loss=0.001204\n",
      "Epoch: 182; train loss=0.001078; validation loss=0.000809\n",
      "Epoch: 183; train loss=0.001061; validation loss=0.000957\n",
      "Epoch: 184; train loss=0.001055; validation loss=0.001026\n",
      "Epoch: 185; train loss=0.001059; validation loss=0.001577\n",
      "Epoch: 186; train loss=0.001106; validation loss=0.001424\n",
      "Epoch: 187; train loss=0.001039; validation loss=0.002268\n",
      "Epoch: 188; train loss=0.001072; validation loss=0.001069\n",
      "Epoch: 189; train loss=0.001063; validation loss=0.000895\n",
      "Epoch: 190; train loss=0.001047; validation loss=0.001110\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m epoch_validation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      2\u001B[0m train_loss_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0003\u001B[39m\n\u001B[1;32m----> 4\u001B[0m train_loss_list, validation_loss_list \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loss_threshold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m plot_loss(train_loss_list, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\GitHub\\NSU_Graduate\\NSU_Graduate\\Pytorch\\Local\\tools\\torch_lib.py:584\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(epoch_count, model, optimizer, loss_function, train_loader, test_loader, epoch_validation, train_loss_threshold)\u001B[0m\n\u001B[0;32m    581\u001B[0m validation_loss_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m()\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_count \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_count):\n\u001B[1;32m--> 584\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    585\u001B[0m     train_loss_list\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch_validation:\n",
      "File \u001B[1;32mC:\\GitHub\\NSU_Graduate\\NSU_Graduate\\Pytorch\\Local\\tools\\torch_lib.py:540\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m--> 540\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    542\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m epoch_loss \u001B[38;5;241m/\u001B[39m num_batches  \u001B[38;5;66;03m# get average loss\u001B[39;00m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m epoch_loss\n",
      "File \u001B[1;32mC:\\GitHub\\NSU_Graduate\\NSU_Graduate\\Pytorch\\Local\\tools\\torch_lib.py:530\u001B[0m, in \u001B[0;36mtrain_loop.<locals>.train_step\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m    527\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# zero gradients to prevent accumulated gradient value\u001B[39;00m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;66;03m# Compute prediction and loss\u001B[39;00m\n\u001B[1;32m--> 530\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\n\u001B[0;32m    533\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[85], line 39\u001B[0m, in \u001B[0;36mLinearModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     36\u001B[0m y \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m lin, act \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinears, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivations):\n\u001B[1;32m---> 39\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mlin\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     y \u001B[38;5;241m=\u001B[39m act(y)\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epoch_validation = True\n",
    "train_loss_threshold = 0.0003\n",
    "\n",
    "train_loss_list, validation_loss_list = train_model(epoch_count, model, optimizer, loss_function, train_loader, validation_loader, True, train_loss_threshold)\n",
    "plot_loss(train_loss_list, \"train loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = test_loop(test_loader, model, loss_function)\n",
    "print(f\"test loss={test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss(validation_loss_list, \"test loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _, (X, y) in enumerate(train_loader):\n",
    "    print(model(X))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_predictions(outputs, full_dataset_loader, linear_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot_actual_predictions(outputs, full_inference_dataset_loader, linear_model, attributes_transform_dict, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_relative_errors(outputs, full_dataset_loader, model, attributes_transform_dict,\n",
    "                     df, 0.01, device, plots_dir, mode='default+hist', bin_count=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot test predictions:\n",
    "plot_relative_errors(outputs, test_loader, model, attributes_transform_dict,\n",
    "                     df, 0.01, device, plots_dir + test_plots_dir, mode='default+hist', bin_count=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### check predictions manually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor = Predictor(full_dataset_loader, df, attributes_transform_dict, model, inputs, outputs)\n",
    "predictions_dict, actuals_dict = predictor.predict(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_prediction(idx: int, prediction_dict, actuals_dict, attribute):\n",
    "    predicted = prediction_dict[attribute][idx]\n",
    "    actual = actuals_dict[attribute][idx]\n",
    "    relative_error = abs(actual - predicted) / actual\n",
    "    print(f\"{idx}: predicted={predicted}; actual={actual}; relative error={relative_error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.to(cpu)    # attach model to cpu before scripting and saving to prevent cuda meta information saved\n",
    "scripted_model = torch.jit.script(model)\n",
    "model_file_name = \"saved_models/\" + model_name + str(round(test_loss, 7)).replace('.', '_')\n",
    "\n",
    "scripted_model.save(model_file_name + \".pt\") # save torch script model which compatible with pytorch c++ api\n",
    "torch.save(model, model_file_name + \".pth\")   # save model in python services specific format\n",
    "\n",
    "# attach model back to device:\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scripted_model(torch.tensor([0.6, 0.362372, 0.04]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(torch.tensor([0.6, 0.362372, 0.04], device=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}