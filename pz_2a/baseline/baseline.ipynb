{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tools.torch_lib import *\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "device = cpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = gpu\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset/\"\n",
    "dataset_file_name = \"pz_2a.csv\"\n",
    "plots_dir = \"plots/\"\n",
    "test_plots_dir = \"test_plots/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "   ro_well  ro_formation       rok  r_well  lambda1\n0     0.01      0.100000  0.119654    0.04      1.0\n1     0.01      0.158489  0.204781    0.04      1.0\n2     0.01      0.251189  0.350298    0.04      1.0\n3     0.01      0.398107  0.590140    0.04      1.0\n4     0.01      0.630957  0.967633    0.04      1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ro_well</th>\n      <th>ro_formation</th>\n      <th>rok</th>\n      <th>r_well</th>\n      <th>lambda1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.01</td>\n      <td>0.100000</td>\n      <td>0.119654</td>\n      <td>0.04</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.01</td>\n      <td>0.158489</td>\n      <td>0.204781</td>\n      <td>0.04</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.01</td>\n      <td>0.251189</td>\n      <td>0.350298</td>\n      <td>0.04</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.01</td>\n      <td>0.398107</td>\n      <td>0.590140</td>\n      <td>0.04</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.01</td>\n      <td>0.630957</td>\n      <td>0.967633</td>\n      <td>0.04</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir + dataset_file_name)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ro_well', 'ro_formation', 'rok', 'r_well', 'lambda1'], dtype='object')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# print attribute's min max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro_well: min=0.01 max=1000.0\n",
      "ro_formation: min=0.1 max=10000.0\n",
      "rok: min=0.0944038 max=21496.2\n",
      "r_well: min=0.04 max=0.2\n",
      "lambda1: min=1.0 max=5.0\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{column}: min={df[column].min()} max={df[column].max()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro_well: min=-4.605170185988091 max=6.907755278982137\n",
      "ro_formation: min=-2.3025850929940455 max=9.210340371976184\n",
      "rok: min=-2.360173952403574 max=9.975631454308614\n",
      "r_well: min=-3.2188758248682006 max=-1.6094379124341003\n",
      "lambda1: min=0.0 max=1.6094379124341003\n"
     ]
    }
   ],
   "source": [
    "# attributes in logarithmic scale:\n",
    "for column in df.columns:\n",
    "    if column == 'd_well':\n",
    "        continue\n",
    "    print(f\"{column}: min={np.log(df[column].min())} max={np.log(df[column].max())}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add dataframe transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "inputs = np.array(['ro_well', 'ro_formation', 'r_well', 'lambda1'])\n",
    "outputs = np.array(['rok']) # 'A02M01N' dropped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "        ro_well  ro_formation       rok  r_well  lambda1\n0           0.0          0.00  0.019214     0.0      0.0\n1           0.0          0.04  0.062773     0.0      0.0\n2           0.0          0.08  0.106292     0.0      0.0\n3           0.0          0.12  0.148574     0.0      0.0\n4           0.0          0.16  0.188660     0.0      0.0\n...         ...           ...       ...     ...      ...\n487573      1.0          0.84  0.828126     1.0      1.0\n487574      1.0          0.88  0.865705     1.0      1.0\n487575      1.0          0.92  0.901307     1.0      1.0\n487576      1.0          0.96  0.934697     1.0      1.0\n487577      1.0          1.00  0.965873     1.0      1.0\n\n[487578 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ro_well</th>\n      <th>ro_formation</th>\n      <th>rok</th>\n      <th>r_well</th>\n      <th>lambda1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.019214</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.04</td>\n      <td>0.062773</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.08</td>\n      <td>0.106292</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.12</td>\n      <td>0.148574</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.16</td>\n      <td>0.188660</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>487573</th>\n      <td>1.0</td>\n      <td>0.84</td>\n      <td>0.828126</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>487574</th>\n      <td>1.0</td>\n      <td>0.88</td>\n      <td>0.865705</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>487575</th>\n      <td>1.0</td>\n      <td>0.92</td>\n      <td>0.901307</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>487576</th>\n      <td>1.0</td>\n      <td>0.96</td>\n      <td>0.934697</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>487577</th>\n      <td>1.0</td>\n      <td>1.00</td>\n      <td>0.965873</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>487578 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logarithmic_columns = ['ro_formation', 'ro_well']\n",
    "# normalize data ('min/max' normalization):\n",
    "interval_th = [-1, 1]     # normalization interval for 'th' activation function\n",
    "interval_leaky_relu = [-0.2, 1]\n",
    "interval_sigmoid = [0, 1] # normalization interval for 'sigmoid' activation function\n",
    "normalize_interval = interval_sigmoid\n",
    "\n",
    "attributes_transform_dict = {}\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# transform output attributes:\n",
    "for output_attr in outputs:\n",
    "    attr_transformer = attributes_transform_dict[output_attr] = AttributeTransformer(df_transformed[output_attr].to_numpy())\n",
    "\n",
    "    # logarithmic transform\n",
    "    forward, backward = np.log, np.exp\n",
    "    df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "    # scaling transform\n",
    "    #forward, backward = get_standard_scaler_transform(attr_transformer.data)\n",
    "    #df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "    # # normalize transform\n",
    "    forward, backward = get_normalize_transforms(attr_transformer.data, normalize_interval)\n",
    "    df_transformed[output_attr] = attr_transformer.transform(forward, backward)\n",
    "\n",
    "# logarithm resistance:\n",
    "for col in logarithmic_columns:\n",
    "    if col in outputs:\n",
    "        continue\n",
    "    df_transformed[col] = df_transformed[col].apply(np.log)\n",
    "\n",
    "# add normalization\n",
    "for attribute in df_transformed.columns:\n",
    "    if attribute in outputs:\n",
    "        continue\n",
    "    transform, _ = get_normalize_transforms(df_transformed[attribute].to_numpy(), normalize_interval)\n",
    "    #transform, _ = get_standard_scaler_transform(df_transformed[attribute].to_numpy())  # use scaling instead of min-max norm\n",
    "    df_transformed[attribute] = transform(df_transformed[attribute].to_numpy())\n",
    "\n",
    "df_transformed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def print_inference_statistic(attributes, df_):\n",
    "    means = []\n",
    "    stds = []\n",
    "    mins = []\n",
    "    maxes = []\n",
    "\n",
    "    for column in attributes:\n",
    "        col_data = df_[column].to_numpy()\n",
    "\n",
    "        if column in logarithmic_columns or column in outputs:\n",
    "            col_data = np.log(col_data)  # first transform - log\n",
    "\n",
    "        # col_mean = np.mean(col_data)\n",
    "        # col_std = np.std(col_data)\n",
    "\n",
    "        # means.append(col_mean)\n",
    "        # stds.append(col_std)\n",
    "        #\n",
    "        # col_data = (col_data - col_mean) / col_std\n",
    "\n",
    "        mins.append(np.min(col_data))\n",
    "        maxes.append(np.max(col_data))\n",
    "\n",
    "    # print(f\"means={means}\")\n",
    "    # print(f\"stds={stds}\")\n",
    "    print(f\"mins={mins}\")\n",
    "    print(f\"maxes={maxes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins=[-4.605170185988091, -2.3025850929940455, 0.04, 1.0]\n",
      "maxes=[6.907755278982137, 9.210340371976184, 0.2, 5.0]\n"
     ]
    }
   ],
   "source": [
    "print_inference_statistic(inputs, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins=[-2.360173952403574]\n",
      "maxes=[9.975631454308614]\n"
     ]
    }
   ],
   "source": [
    "print_inference_statistic(outputs, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Datasets and create dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, df_, inputs, outputs, device):\n",
    "        self.df = df_\n",
    "        self.inputs = torch.from_numpy(df_[inputs].to_numpy()).float().to(device)\n",
    "        self.outputs = torch.from_numpy(df_[outputs].to_numpy()).float().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, label = self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "        return item, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "batch_size = 20000\n",
    "\n",
    "train_df, test_df = train_test_split(df_transformed, shuffle=True, test_size=0.3)\n",
    "test_df, validation_df = train_test_split(test_df, shuffle=True, test_size=0.33)\n",
    "\n",
    "train_dataset = SimpleDataset(train_df, inputs, outputs, device)\n",
    "test_dataset = SimpleDataset(test_df, inputs, outputs, device)\n",
    "validation_dataset = SimpleDataset(validation_df, inputs, outputs, device)\n",
    "full_dataset = SimpleDataset(df_transformed, inputs, outputs, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "full_dataset_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class WeightedMAE(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedMAE, self).__init__()\n",
    "        self.mae = nn.L1Loss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        weighted_inputs = inputs * self.weights\n",
    "\n",
    "        return self.mae(weighted_inputs, targets)\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.weights = self.weights.to(device)\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        module_list = []\n",
    "        for i in range(layers_count - 1):\n",
    "            module_list.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n",
    "        module_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "\n",
    "        activations_list = []\n",
    "        for i in range(layers_count):\n",
    "            activations_list.append(activations[act_str_list[i]])\n",
    "\n",
    "        self.linears = nn.ModuleList(module_list)\n",
    "        self.activations = nn.ModuleList(activations_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act in zip(self.linears, self.activations):\n",
    "            y = lin(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class LinearLNormModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        linears_list = []\n",
    "        layers_norm_list = []\n",
    "\n",
    "        for i in range(layers_count - 1):\n",
    "            in_features, out_features = layers_dims[i], layers_dims[i + 1]\n",
    "            linears_list.append(nn.Linear(in_features, out_features))\n",
    "            layers_norm_list.append(nn.LayerNorm(out_features))\n",
    "        # add last layer\n",
    "        linears_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "        layers_norm_list.append(nn.LayerNorm(output_dim))\n",
    "\n",
    "        self.linears = nn.ModuleList(linears_list)\n",
    "        self.activations = nn.ModuleList([activations[act_str_list[i]] for i in range(len(act_str_list))])\n",
    "        self.layer_normalizations = nn.ModuleList(layers_norm_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act, norm in zip(self.linears, self.activations, self.layer_normalizations):\n",
    "            y = lin(y)\n",
    "            y = norm(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# add batch normalization\n",
    "class LinearBNormModel(nn.Module):\n",
    "    def __init__(self, layers_dims, act_str_list, output_dim):\n",
    "        super().__init__()\n",
    "        layers_count = len(layers_dims)\n",
    "        assert layers_count > 0\n",
    "\n",
    "        linears_list = []\n",
    "        batch_norm_list = []\n",
    "\n",
    "        for i in range(layers_count - 1):\n",
    "            in_features, out_features = layers_dims[i], layers_dims[i + 1]\n",
    "            linears_list.append(nn.Linear(in_features, out_features))\n",
    "            batch_norm_list.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "        linears_list.append(nn.Linear(layers_dims[layers_count - 1], output_dim))\n",
    "        batch_norm_list.append(nn.BatchNorm1d(output_dim))\n",
    "\n",
    "        activations_list = []\n",
    "        for i in range(layers_count):\n",
    "            activations_list.append(activations[act_str_list[i]])\n",
    "\n",
    "        self.linears = nn.ModuleList(linears_list)\n",
    "        self.activations = nn.ModuleList(activations_list)\n",
    "        self.batch_normalizations = nn.ModuleList(batch_norm_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for lin, act, norm in zip(self.linears, self.activations, self.batch_normalizations):\n",
    "            y = lin(y)\n",
    "            y = norm(y)\n",
    "            y = act(y)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearModel(\n  (linears): ModuleList(\n    (0): Linear(in_features=4, out_features=40, bias=True)\n    (1): Linear(in_features=40, out_features=150, bias=True)\n    (2): Linear(in_features=150, out_features=1500, bias=True)\n    (3): Linear(in_features=1500, out_features=150, bias=True)\n    (4): Linear(in_features=150, out_features=10, bias=True)\n    (5): Linear(in_features=10, out_features=1, bias=True)\n  )\n  (activations): ModuleList(\n    (0-5): 6 x LeakyReLU(negative_slope=0.01)\n  )\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = [len(inputs), 40, 150, 1500, 150, 10]\n",
    "layers_count = len(layers_dims)\n",
    "activations_string_list = ['leaky-relu' for i in range(layers_count)]\n",
    "#activations_string_list[-1] = 'sigmoid'\n",
    "\n",
    "linear_model = LinearModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "#linear_bn_model = LinearBNormModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "#linear_ln_model = LinearLNormModel(layers_dims, activations_string_list, len(outputs)).to(device)\n",
    "\n",
    "model = linear_model\n",
    "model_name = \"linear_model\"\n",
    "linear_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epoch_count = 200\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#loss_function = WeightedMAE(torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], dtype=float))\n",
    "loss_function = nn.L1Loss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; train loss=0.246051; validation loss=0.209986\n",
      "Epoch: 1; train loss=0.185595; validation loss=0.171099\n",
      "Epoch: 2; train loss=0.155382; validation loss=0.136974\n",
      "Epoch: 3; train loss=0.110655; validation loss=0.080052\n",
      "Epoch: 4; train loss=0.065023; validation loss=0.053110\n",
      "Epoch: 5; train loss=0.045475; validation loss=0.039157\n",
      "Epoch: 6; train loss=0.034947; validation loss=0.030494\n",
      "Epoch: 7; train loss=0.027528; validation loss=0.025657\n",
      "Epoch: 8; train loss=0.022680; validation loss=0.020428\n",
      "Epoch: 9; train loss=0.018764; validation loss=0.016958\n",
      "Epoch: 10; train loss=0.015508; validation loss=0.013455\n",
      "Epoch: 11; train loss=0.012604; validation loss=0.011841\n",
      "Epoch: 12; train loss=0.010632; validation loss=0.009801\n",
      "Epoch: 13; train loss=0.008804; validation loss=0.008763\n",
      "Epoch: 14; train loss=0.007942; validation loss=0.007363\n",
      "Epoch: 15; train loss=0.006936; validation loss=0.005948\n",
      "Epoch: 16; train loss=0.006156; validation loss=0.006437\n",
      "Epoch: 17; train loss=0.005584; validation loss=0.007028\n",
      "Epoch: 18; train loss=0.005360; validation loss=0.005362\n",
      "Epoch: 19; train loss=0.005030; validation loss=0.004898\n",
      "Epoch: 20; train loss=0.004753; validation loss=0.004464\n",
      "Epoch: 21; train loss=0.004522; validation loss=0.004672\n",
      "Epoch: 22; train loss=0.004381; validation loss=0.003917\n",
      "Epoch: 23; train loss=0.004155; validation loss=0.004683\n",
      "Epoch: 24; train loss=0.004051; validation loss=0.004513\n",
      "Epoch: 25; train loss=0.004047; validation loss=0.004239\n",
      "Epoch: 26; train loss=0.003946; validation loss=0.004264\n",
      "Epoch: 27; train loss=0.003887; validation loss=0.003673\n",
      "Epoch: 28; train loss=0.003794; validation loss=0.003629\n",
      "Epoch: 29; train loss=0.003717; validation loss=0.003830\n",
      "Epoch: 30; train loss=0.003656; validation loss=0.003609\n",
      "Epoch: 31; train loss=0.003600; validation loss=0.003561\n",
      "Epoch: 32; train loss=0.003552; validation loss=0.003164\n",
      "Epoch: 33; train loss=0.003473; validation loss=0.003497\n",
      "Epoch: 34; train loss=0.003426; validation loss=0.003711\n",
      "Epoch: 35; train loss=0.003425; validation loss=0.003582\n",
      "Epoch: 36; train loss=0.003380; validation loss=0.003970\n",
      "Epoch: 37; train loss=0.003264; validation loss=0.003077\n",
      "Epoch: 38; train loss=0.003322; validation loss=0.003454\n",
      "Epoch: 39; train loss=0.003286; validation loss=0.003450\n",
      "Epoch: 40; train loss=0.003257; validation loss=0.003396\n",
      "Epoch: 41; train loss=0.003263; validation loss=0.002990\n",
      "Epoch: 42; train loss=0.003190; validation loss=0.003160\n",
      "Epoch: 43; train loss=0.003195; validation loss=0.003222\n",
      "Epoch: 44; train loss=0.003197; validation loss=0.003126\n",
      "Epoch: 45; train loss=0.003134; validation loss=0.003150\n",
      "Epoch: 46; train loss=0.003100; validation loss=0.003536\n",
      "Epoch: 47; train loss=0.003005; validation loss=0.003137\n",
      "Epoch: 48; train loss=0.003095; validation loss=0.003521\n",
      "Epoch: 49; train loss=0.003064; validation loss=0.002992\n",
      "Epoch: 50; train loss=0.003055; validation loss=0.002897\n",
      "Epoch: 51; train loss=0.002975; validation loss=0.003386\n",
      "Epoch: 52; train loss=0.002994; validation loss=0.002784\n",
      "Epoch: 53; train loss=0.002974; validation loss=0.003186\n",
      "Epoch: 54; train loss=0.002964; validation loss=0.002687\n",
      "Epoch: 55; train loss=0.002893; validation loss=0.003007\n",
      "Epoch: 56; train loss=0.002928; validation loss=0.003063\n",
      "Epoch: 57; train loss=0.002916; validation loss=0.003056\n",
      "Epoch: 58; train loss=0.002896; validation loss=0.002709\n",
      "Epoch: 59; train loss=0.002859; validation loss=0.003149\n",
      "Epoch: 60; train loss=0.002868; validation loss=0.002958\n",
      "Epoch: 61; train loss=0.002835; validation loss=0.003025\n",
      "Epoch: 62; train loss=0.002852; validation loss=0.002828\n",
      "Epoch: 63; train loss=0.002824; validation loss=0.002802\n",
      "Epoch: 64; train loss=0.002775; validation loss=0.003073\n",
      "Epoch: 65; train loss=0.002810; validation loss=0.002571\n",
      "Epoch: 66; train loss=0.002718; validation loss=0.002729\n",
      "Epoch: 67; train loss=0.002757; validation loss=0.002909\n",
      "Epoch: 68; train loss=0.002756; validation loss=0.002614\n",
      "Epoch: 69; train loss=0.002710; validation loss=0.002449\n",
      "Epoch: 70; train loss=0.002653; validation loss=0.002828\n",
      "Epoch: 71; train loss=0.002702; validation loss=0.002699\n",
      "Epoch: 72; train loss=0.002650; validation loss=0.002831\n",
      "Epoch: 73; train loss=0.002643; validation loss=0.002587\n",
      "Epoch: 74; train loss=0.002641; validation loss=0.002706\n",
      "Epoch: 75; train loss=0.002600; validation loss=0.002705\n",
      "Epoch: 76; train loss=0.002625; validation loss=0.002694\n",
      "Epoch: 77; train loss=0.002585; validation loss=0.002710\n",
      "Epoch: 78; train loss=0.002571; validation loss=0.002582\n",
      "Epoch: 79; train loss=0.002589; validation loss=0.002803\n",
      "Epoch: 80; train loss=0.002590; validation loss=0.002442\n",
      "Epoch: 81; train loss=0.002550; validation loss=0.002582\n",
      "Epoch: 82; train loss=0.002525; validation loss=0.002464\n",
      "Epoch: 83; train loss=0.002522; validation loss=0.002581\n",
      "Epoch: 84; train loss=0.002503; validation loss=0.002721\n",
      "Epoch: 85; train loss=0.002473; validation loss=0.002376\n",
      "Epoch: 86; train loss=0.002509; validation loss=0.002330\n",
      "Epoch: 87; train loss=0.002444; validation loss=0.002764\n",
      "Epoch: 88; train loss=0.002457; validation loss=0.002442\n",
      "Epoch: 89; train loss=0.002447; validation loss=0.002426\n",
      "Epoch: 90; train loss=0.002434; validation loss=0.002513\n",
      "Epoch: 91; train loss=0.002397; validation loss=0.002514\n",
      "Epoch: 92; train loss=0.002418; validation loss=0.002389\n",
      "Epoch: 93; train loss=0.002428; validation loss=0.002389\n",
      "Epoch: 94; train loss=0.002366; validation loss=0.002355\n",
      "Epoch: 95; train loss=0.002388; validation loss=0.002343\n",
      "Epoch: 96; train loss=0.002369; validation loss=0.002304\n",
      "Epoch: 97; train loss=0.002373; validation loss=0.002293\n",
      "Epoch: 98; train loss=0.002323; validation loss=0.002220\n",
      "Epoch: 99; train loss=0.002322; validation loss=0.002563\n",
      "Epoch: 100; train loss=0.002300; validation loss=0.002235\n",
      "Epoch: 101; train loss=0.002316; validation loss=0.002288\n",
      "Epoch: 102; train loss=0.002309; validation loss=0.002322\n",
      "Epoch: 103; train loss=0.002275; validation loss=0.002275\n",
      "Epoch: 104; train loss=0.002307; validation loss=0.002027\n",
      "Epoch: 105; train loss=0.002235; validation loss=0.002182\n",
      "Epoch: 106; train loss=0.002243; validation loss=0.002251\n",
      "Epoch: 107; train loss=0.002254; validation loss=0.002173\n",
      "Epoch: 108; train loss=0.002229; validation loss=0.002068\n",
      "Epoch: 109; train loss=0.002207; validation loss=0.002266\n",
      "Epoch: 110; train loss=0.002213; validation loss=0.002027\n",
      "Epoch: 111; train loss=0.002177; validation loss=0.002201\n",
      "Epoch: 112; train loss=0.002184; validation loss=0.002144\n",
      "Epoch: 113; train loss=0.002179; validation loss=0.002290\n",
      "Epoch: 114; train loss=0.002166; validation loss=0.002121\n",
      "Epoch: 115; train loss=0.002160; validation loss=0.002046\n",
      "Epoch: 116; train loss=0.002129; validation loss=0.002210\n",
      "Epoch: 117; train loss=0.002139; validation loss=0.002110\n",
      "Epoch: 118; train loss=0.002146; validation loss=0.002307\n",
      "Epoch: 119; train loss=0.002137; validation loss=0.002120\n",
      "Epoch: 120; train loss=0.002115; validation loss=0.002176\n",
      "Epoch: 121; train loss=0.002102; validation loss=0.002177\n",
      "Epoch: 122; train loss=0.002122; validation loss=0.002106\n",
      "Epoch: 123; train loss=0.002095; validation loss=0.002157\n",
      "Epoch: 124; train loss=0.002075; validation loss=0.002184\n",
      "Epoch: 125; train loss=0.002073; validation loss=0.002015\n",
      "Epoch: 126; train loss=0.002072; validation loss=0.002026\n",
      "Epoch: 127; train loss=0.002048; validation loss=0.001968\n",
      "Epoch: 128; train loss=0.002051; validation loss=0.002003\n",
      "Epoch: 129; train loss=0.002034; validation loss=0.002028\n",
      "Epoch: 130; train loss=0.002040; validation loss=0.002073\n",
      "Epoch: 131; train loss=0.002031; validation loss=0.001981\n",
      "Epoch: 132; train loss=0.002029; validation loss=0.001980\n",
      "Epoch: 133; train loss=0.002022; validation loss=0.002013\n",
      "Epoch: 134; train loss=0.002005; validation loss=0.001973\n",
      "Epoch: 135; train loss=0.001999; validation loss=0.001896\n",
      "Epoch: 136; train loss=0.001998; validation loss=0.001886\n",
      "Epoch: 137; train loss=0.002000; validation loss=0.001969\n",
      "Epoch: 138; train loss=0.001978; validation loss=0.002046\n",
      "Epoch: 139; train loss=0.001978; validation loss=0.001929\n",
      "Epoch: 140; train loss=0.001973; validation loss=0.001906\n",
      "Epoch: 141; train loss=0.001950; validation loss=0.001899\n",
      "Epoch: 142; train loss=0.001922; validation loss=0.001828\n",
      "Epoch: 143; train loss=0.001926; validation loss=0.001925\n",
      "Epoch: 144; train loss=0.001916; validation loss=0.001920\n",
      "Epoch: 145; train loss=0.001943; validation loss=0.002007\n",
      "Epoch: 146; train loss=0.001920; validation loss=0.002037\n",
      "Epoch: 147; train loss=0.001915; validation loss=0.001742\n",
      "Epoch: 148; train loss=0.001897; validation loss=0.001665\n",
      "Epoch: 149; train loss=0.001872; validation loss=0.001875\n",
      "Epoch: 150; train loss=0.001887; validation loss=0.001837\n",
      "Epoch: 151; train loss=0.001884; validation loss=0.001783\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m epoch_validation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      2\u001B[0m train_loss_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0003\u001B[39m\n\u001B[1;32m----> 4\u001B[0m train_loss_list, validation_loss_list \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loss_threshold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m plot_loss(train_loss_list, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\GitHub\\NSU_Graduate\\NSU_Graduate\\Pytorch\\Local\\tools\\torch_lib.py:587\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(epoch_count, model, optimizer, loss_function, train_loader, test_loader, epoch_validation, train_loss_threshold)\u001B[0m\n\u001B[0;32m    584\u001B[0m train_loss_list\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch_validation:\n\u001B[1;32m--> 587\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtest_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m     validation_loss_list\u001B[38;5;241m.\u001B[39mappend(test_loss)\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; train loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m>8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; validation loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m>8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\GitHub\\NSU_Graduate\\NSU_Graduate\\Pytorch\\Local\\tools\\torch_lib.py:568\u001B[0m, in \u001B[0;36mtest_loop\u001B[1;34m(dataloader, model, loss_fn)\u001B[0m\n\u001B[0;32m    565\u001B[0m test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():  \u001B[38;5;66;03m# we test model, so it's no need to calc gradients\u001B[39;00m\n\u001B[1;32m--> 568\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m    569\u001B[0m         pred \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[0;32m    570\u001B[0m         test_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mC:\\Python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[12], line 10\u001B[0m, in \u001B[0;36mSimpleDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__len__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs)\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m     11\u001B[0m     item, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs[idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs[idx]\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m item, label\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epoch_validation = True\n",
    "train_loss_threshold = 0.0003\n",
    "\n",
    "train_loss_list, validation_loss_list = train_model(epoch_count, model, optimizer, loss_function, train_loader, validation_loader, True, train_loss_threshold)\n",
    "plot_loss(train_loss_list, \"train loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = test_loop(test_loader, model, loss_function)\n",
    "print(f\"test loss={test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss(validation_loss_list, \"test loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _, (X, y) in enumerate(train_loader):\n",
    "    print(model(X))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_predictions(outputs, full_dataset_loader, linear_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot_actual_predictions(outputs, full_inference_dataset_loader, linear_model, attributes_transform_dict, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearModel(\n  (linears): ModuleList(\n    (0): Linear(in_features=4, out_features=40, bias=True)\n    (1): Linear(in_features=40, out_features=120, bias=True)\n    (2): Linear(in_features=120, out_features=1200, bias=True)\n    (3): Linear(in_features=1200, out_features=120, bias=True)\n    (4): Linear(in_features=120, out_features=10, bias=True)\n    (5): Linear(in_features=10, out_features=1, bias=True)\n  )\n  (activations): ModuleList(\n    (0-5): 6 x LeakyReLU(negative_slope=0.01)\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"saved_models/\" + \"linear_model0_0007086.pth\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "plot_relative_errors(outputs, full_dataset_loader, model, attributes_transform_dict,\n",
    "                     df, 0.01, device, plots_dir, mode='default+hist', bin_count=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot test predictions:\n",
    "plot_relative_errors(outputs, test_loader, model, attributes_transform_dict,\n",
    "                     df, 0.01, device, plots_dir + test_plots_dir, mode='default+hist', bin_count=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### check predictions manually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor = Predictor(full_dataset_loader, df, attributes_transform_dict, model, inputs, outputs)\n",
    "predictions_dict, actuals_dict = predictor.predict(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_prediction(idx: int, prediction_dict, actuals_dict, attribute):\n",
    "    predicted = prediction_dict[attribute][idx]\n",
    "    actual = actuals_dict[attribute][idx]\n",
    "    relative_error = abs(actual - predicted) / actual\n",
    "    print(f\"{idx}: predicted={predicted}; actual={actual}; relative error={relative_error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.to(cpu)    # attach model to cpu before scripting and saving to prevent cuda meta information saved\n",
    "scripted_model = torch.jit.script(model)\n",
    "models_dir = \"saved_models/\"\n",
    "model_file_name = models_dir + model_name + str(round(test_loss, 7)).replace('.', '_')\n",
    "\n",
    "scripted_model.save(model_file_name + \".pt\") # save torch script model which compatible with pytorch c++ api\n",
    "torch.save(model, model_file_name + \".pth\")   # save model in python services specific format\n",
    "\n",
    "# attach model back to device:\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scripted_model(torch.tensor([0.6, 0.362372, 0.04]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(torch.tensor([0.6, 0.362372, 0.04], device=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check non linearly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = torch.load(\"saved_models/\" + \"linear_model0_0007086.pth\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_idx = 3\n",
    "layer_idx = 0\n",
    "for i in range(batch_idx + 1):\n",
    "    batch = next(iter(full_dataset_loader))\n",
    "batch = batch[0]\n",
    "\n",
    "plot_layer_distribution(model, layer_idx, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check classic ML models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "class LinearRegressionModel(object):\n",
    "    def __init__(self, n_jobs:int = 10):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.model = LinearRegression(n_jobs=n_jobs)\n",
    "        self.degree = 2\n",
    "\n",
    "    def fit(self, train_df_, inputs_, outputs_, degree: int = 2):\n",
    "        train_data_X = train_df_[inputs_].to_numpy()\n",
    "        train_data_Y = train_df_[outputs_].to_numpy()\n",
    "\n",
    "        polynomial_transform = PolynomialFeatures(degree=degree)\n",
    "        train_data_X_transformed = polynomial_transform.fit_transform(train_data_X)\n",
    "\n",
    "        self.model.fit(train_data_X_transformed, train_data_Y)\n",
    "        self.degree = degree\n",
    "\n",
    "    def predict(self, X_data, do_poly_transform: bool = True):\n",
    "        input_X = X_data\n",
    "\n",
    "        if do_poly_transform:\n",
    "            polynomial_transform = PolynomialFeatures(degree=self.degree)\n",
    "            input_X = polynomial_transform.fit_transform(input_X)\n",
    "\n",
    "        return self.model.predict(input_X)\n",
    "\n",
    "    def test(self, X_data, Y_actual, do_poly_transform: bool = True):\n",
    "        predicted = self.predict(X_data, do_poly_transform)\n",
    "        loss_ = torch.nn.L1Loss()\n",
    "\n",
    "        return loss_(torch.tensor(predicted), torch.tensor(Y_actual))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_relative_error_arrays(actuals, predictions, outputs,\n",
    "                                   transform_dict, model_name: str,\n",
    "                                   threshold: float = 0.05, width: int = 10000,\n",
    "                                   height: int = 500, with_shader=True):\n",
    "    for i in range(len(outputs)):\n",
    "        output_attr = outputs[i]\n",
    "        transformer = transform_dict[output_attr]\n",
    "\n",
    "        # transform predictions back:\n",
    "        prediction = predictions[:, i]\n",
    "        transformer.set_data(prediction)\n",
    "        final_prediction = torch.from_numpy(transformer.transform_backward())\n",
    "\n",
    "        # get actual data from dataloader\n",
    "        actual = actuals[:, i]\n",
    "        transformer.set_data(actual)\n",
    "        actual = torch.from_numpy(transformer.transform_backward())\n",
    "\n",
    "        # plot graphic\n",
    "        if with_shader:\n",
    "            fig = plot_relative_error_shader(actual, final_prediction, threshold, 'relative error ' + output_attr, width, height)\n",
    "        else:\n",
    "            fig = plot_relative_error(actual, final_prediction, threshold, 'relative error ' + output_attr, width, height)\n",
    "        fig.show('browser')\n",
    "        fig.write_image(plots_dir + \"err_\" + model_name + \"_\" + output_attr + \".pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_regression_model = LinearRegressionModel()\n",
    "linear_regression_model.fit(train_df, inputs, outputs, degree=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = linear_regression_model.test(df_transformed[inputs].to_numpy(), df_transformed[outputs].to_numpy())\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = linear_regression_model.predict(df_transformed[inputs].to_numpy())\n",
    "predictions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_relative_error_arrays(df_transformed[outputs].to_numpy(), predictions, outputs,\n",
    "                           attributes_transform_dict, \"linear_regression\", threshold=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}